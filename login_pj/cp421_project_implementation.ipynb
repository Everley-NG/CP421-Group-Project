{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098df237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP421 Group Project (Group 3)\n",
    "# -----------------------------------------------------------------\n",
    "# File: cp421_group_project.ipynb\n",
    "# Author: Yvonne Itangishaka, Mariam Lom, Hoi Hin Ng, Melissa Pinto\n",
    "# Due Date: Dec 6th, 2023\n",
    "# -----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c3bfa",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a95135",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Book Recommendation dataset from Kaggle\n",
    "books_data = pd.read_csv('Books.csv')\n",
    "ratings_data = pd.read_csv('Ratings.csv')\n",
    "users_data = pd.read_csv('Users.csv')\n",
    "\n",
    "print(\"Books Data:\")\n",
    "display(books_data.head())\n",
    "print(\"\\nRatings Data:\")\n",
    "display(ratings_data.head())\n",
    "print(\"\\nUsers Data:\")\n",
    "display(users_data.head())\n",
    "\n",
    "# Concatenate relevant columns into 'Books-Data'\n",
    "books_data[\"Books-Data\"] = (\n",
    "    books_data[\"ISBN\"].astype(str) +\n",
    "    books_data[\"Book-Title\"] +\n",
    "    books_data[\"Book-Author\"] +\n",
    "    books_data[\"Year-Of-Publication\"].astype(str) +\n",
    "    books_data[\"Publisher\"] +\n",
    "    books_data[\"Image-URL-S\"] +\n",
    "    books_data[\"Image-URL-M\"] +\n",
    "    books_data[\"Image-URL-L\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075fe36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = pd.read_csv('/content/Books.csv', dtype={'Year-Of-Publication': str})\n",
    "ratings_data = pd.read_csv('/content/Ratings.csv')\n",
    "users_data = pd.read_csv('/content/Users.csv')\n",
    "\n",
    "# Concatenate relevant columns into 'Books-Data' column\n",
    "books_data[\"Books-Data\"] = (\n",
    "    books_data[\"ISBN\"].astype(str) +\n",
    "    books_data[\"Book-Title\"]+\n",
    "    books_data[\"Book-Author\"] +\n",
    "    books_data[\"Year-Of-Publication\"].astype(str) +\n",
    "    books_data[\"Publisher\"] +\n",
    "    books_data[\"Image-URL-S\"] +\n",
    "    books_data[\"Image-URL-M\"] +\n",
    "    books_data[\"Image-URL-L\"]\n",
    ")\n",
    "\n",
    "print(\"Books Data:\")\n",
    "display(books_data.head())\n",
    "print(\"\\nRatings Data:\")\n",
    "display(ratings_data.head())\n",
    "print(\"\\nUsers Data:\")\n",
    "display(users_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015b3a66",
   "metadata": {},
   "source": [
    "## Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8a8744",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_data = books_data.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L']) #drop because it is not needed\n",
    "#Check for missing values\n",
    "missing_books = books_data.isnull().sum()\n",
    "print(\"missing values in books:\\n\",missing_books)\n",
    "print(\"\\n\")\n",
    "missing_ratings = ratings_data.isnull().sum()\n",
    "print(\"missing values in ratings:\\n\",missing_ratings)\n",
    "print(\"\\n\")\n",
    "missing_users = users_data.isnull().sum()\n",
    "print(\"missing values in users:\\n\",missing_users)\n",
    "print(\"\\n\")\n",
    "\n",
    "#Check for duplicates in data set\n",
    "print(\"duplicates in books:\\n\",books_data.duplicated().sum())\n",
    "print(\"\\n\")\n",
    "ratings_books = ratings_data.duplicated().sum()\n",
    "print(\"duplicates in ratings:\\n\",ratings_books)\n",
    "print(\"\\n\")\n",
    "duplicate_users = users_data.duplicated().sum()\n",
    "print(\"duplicates in users:\\n\",duplicate_users)\n",
    "\n",
    "#print ((ratings_data==0).values.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c08bc42",
   "metadata": {},
   "source": [
    "## Merge All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge tables\n",
    "first_merged_data = pd.merge(books_data, ratings_data, on='ISBN', how='inner')\n",
    "merged_data = pd.merge(first_merged_data, users_data, on='User-ID', how='inner')\n",
    "\n",
    "display(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374e1f65",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f5d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations, check for any outliers!\n",
    "\n",
    "#Publication Year\n",
    "valid_years = merged_data['Year-Of-Publication'].astype(str).str.isnumeric()\n",
    "filtered_data = merged_data[valid_years]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(filtered_data['Year-Of-Publication'].astype(int), color='skyblue', fill=True)\n",
    "plt.title('Density Plot of Books Based on Publication Year')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Count Plot of Books for Top 20 Publication Years\n",
    "valid_years = merged_data['Year-Of-Publication'].astype(str).str.isnumeric()\n",
    "filtered_data = merged_data[valid_years]\n",
    "top_years = filtered_data['Year-Of-Publication'].value_counts().nlargest(20).index\n",
    "filtered_data_top_years = filtered_data[filtered_data['Year-Of-Publication'].isin(top_years)]\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(x='Year-Of-Publication', data=filtered_data_top_years, palette='viridis')\n",
    "plt.title('Count Plot of Books for Top 20 Publication Years')\n",
    "plt.xlabel('Publication Year')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n",
    "# Density plot of Average book rating\n",
    "average_ratings = merged_data.groupby('ISBN')['Book-Rating'].mean()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(average_ratings, color='skyblue', fill=True)\n",
    "plt.title('Density Plot of Average Book Ratings')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "#Distribution of Book Ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(merged_data['Book-Rating'], bins=5, edgecolor='black', color='skyblue', alpha=0.7)\n",
    "overall_average_rating = merged_data['Book-Rating'].mean()\n",
    "plt.axvline(x=overall_average_rating, color='red', linestyle='dashed', linewidth=2, label=f'Overall Avg: {overall_average_rating:.2f}')\n",
    "plt.title('Distribution of Book Ratings')\n",
    "plt.xlabel('Book Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Distribution of User rating counts\n",
    "user_ratings_count = merged_data['User-ID'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(user_ratings_count, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of User Ratings')\n",
    "plt.xlabel('Number of Ratings')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.xlim(0, 2000)\n",
    "plt.show()\n",
    "\n",
    "#most popular locations\n",
    "merged_data['Location'] = merged_data['Location'].apply(lambda x:x.split(',')[-1])\n",
    "merged_data['Location'].value_counts().head(10).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fe2c0",
   "metadata": {},
   "source": [
    "## Setting Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187bdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user who has rated at least x books\n",
    "#books that have received at least x ratings ( x average ratings ?) Book ratings>=50\n",
    "#removes outliers\n",
    "merged_data = merged_data.groupby('ISBN').filter(lambda x: x['Book-Rating'].count() >= 50)\n",
    "merged_data = merged_data.groupby('User-ID').filter(lambda x: x['Book-Rating'].count() >= 50)\n",
    "\n",
    "# Display the merged_data after applying the threshold\n",
    "print(\"Merged data after applying thresholds:\")\n",
    "display(merged_data.head())\n",
    "\n",
    "print(merged_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dd317",
   "metadata": {},
   "source": [
    "## KNN Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d088f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_user_mat = merged_data.pivot(index='ISBN', columns='User-ID', values='Book-Rating').fillna(0)\n",
    "book_user_mat_sparse = csr_matrix(book_user_mat.values)\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)\n",
    "model_knn.fit(book_user_mat_sparse)\n",
    "\n",
    "\n",
    "target_book_isbn = '0452264464'  # Replace with the actual ISBN\n",
    "target_book_index = book_user_mat.index.get_loc(target_book_isbn)\n",
    "distances, indices = model_knn.kneighbors([book_user_mat.iloc[target_book_index].values], n_neighbors=5)\n",
    "\n",
    "print(\"Nearest Neighbors for Book with ISBN:\", target_book_isbn)\n",
    "for i, (distance, index) in enumerate(zip(distances.flatten(), indices.flatten())):\n",
    "    neighbor_book_isbn = book_user_mat.index[index]\n",
    "    print(f\"{i + 1}. ISBN: {neighbor_book_isbn}, Distance: {distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa827d9",
   "metadata": {},
   "source": [
    "## Calculation Global Mean, User Mean, Item Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6ebae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculating global,\n",
    "\n",
    "global_mean = merged_data['Book-Rating'].mean()\n",
    "\n",
    "# Test\n",
    "print(\"Global Mean:\", global_mean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a4512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_means = merged_data.groupby('User-ID')['Book-Rating'].mean()\n",
    "\n",
    "# Test\n",
    "user_id = 11676  # Replace with the desired user ID\n",
    "user_mean = user_means.get(user_id, global_mean)  # Use global mean if user ID is not found\n",
    "print(f\"User {user_id} Mean Rating:\", user_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da396e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_means = merged_data.groupby('ISBN')['Book-Rating'].mean()\n",
    "\n",
    "# Test\n",
    "isbn = '0440234743'  # Replace with the desired ISBN\n",
    "item_mean = item_means.get(isbn, global_mean)  # Use global mean if ISBN is not found\n",
    "print(f\"Item {isbn} Mean Rating:\", item_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129543b",
   "metadata": {},
   "source": [
    "## Spliting data into Training/Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931f856",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
    "# Create user-item matrices training and testing sets\n",
    "trainset_matrix = trainset.pivot_table(index='User-ID', columns='ISBN', values='Book-Rating', fill_value=0)\n",
    "testset_matrix = testset.pivot_table(index='User-ID', columns='ISBN', values='Book-Rating', fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e2b707",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b153d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFilteringRecommendationSystem:\n",
    "    def __init__(self, user_Item, global_mean, user_means, item_means):\n",
    "        # Parameter Initialization and Pearson Calcumations\n",
    "        self.user_Item = user_Item\n",
    "        self.global_mean = global_mean\n",
    "        self.user_means = user_means\n",
    "        self.item_means = item_means\n",
    "        self.userPearsonSim = 1 - pairwise_distances(user_Item, metric='correlation')\n",
    "        self.itemPearsonSim = 1 - pairwise_distances(user_Item.T, metric='correlation')\n",
    "\n",
    "    def userBased_predict(self, user_id, item_id):\n",
    "      # Gettting specifi user ratings\n",
    "        user_ratings = self.user_Item.loc[user_id]\n",
    "        # Getting the similaries of the user items\n",
    "        ratingsSimValue = pd.Series(self.itemPearsonSim[self.user_Item.columns.get_loc(item_id)], index=self.user_Item.columns)\n",
    "        # Filter out the user's ratings from the similarity scores\n",
    "        filter_ratingsSimValue = np.multiply(ratingsSimValue, user_ratings)\n",
    "        user_mean = self.user_means.get(user_id, self.global_mean)\n",
    "        # Adding 1 to handle zero similarity scores\n",
    "        # returns the user predictions\n",
    "        itemUserRating_prediction = (filter_ratingsSimValue.sum() + user_mean) / (ratingsSimValue.abs().sum() + 1)\n",
    "        return itemUserRating_prediction\n",
    "\n",
    "    def itemBased_predict(self, user_id, item_id):\n",
    "         # Gettting specific user ratings to a specific book\n",
    "        user_ratings = self.user_Item.loc[user_id]\n",
    "        ratingsSimValue = pd.Series(self.userPearsonSim[user_id - 1], index=self.user_Item.index)\n",
    "        filter_ratingsSimValue = np.multiply(ratingsSimValue, user_ratings)\n",
    "        item_mean = self.item_means.get(item_id, self.global_mean)\n",
    "        # Adding 1 to handle zero similarity scores\n",
    "        # returns the itemspredictions\n",
    "        itemUserRating_prediction = (filter_ratingsSimValue.sum() + item_mean) / (ratingsSimValue.abs().sum() + 1)\n",
    "        return itemUserRating_prediction\n",
    "\n",
    "    def bookUser_Recommender(self, user_id, top_n=10):\n",
    "        itemRatings_prediction = {}\n",
    "        # Looping through the recommendations to generate top n recommendations\n",
    "        for item_id in self.user_Item.columns:\n",
    "            itemUserRating_prediction = self.userBased_predict(user_id, item_id)\n",
    "            itemRatings_prediction[item_id] = itemUserRating_prediction\n",
    "            # Generates the top n recommendations\n",
    "        bestBook_recommendations = sorted(itemRatings_prediction.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        return bestBook_recommendations\n",
    "\n",
    "    def evaluate_recommendations(self, user_id, testset_matrix, threshold=0.1):\n",
    "        # Generate recommendations for the user\n",
    "        recommended_books = self.bookUser_Recommender(user_id)\n",
    "        recommended_isbns, predicted_ratings = zip(*recommended_books)\n",
    "\n",
    "        # Extract actual ratings for recommended books\n",
    "        actual_ratings = testset_matrix.loc[user_id, testset_matrix.columns.isin(recommended_isbns)]\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = mean_squared_error(actual_ratings, predicted_ratings, squared=False)\n",
    "\n",
    "        # Calculate Precision, Recall, and F1-score\n",
    "        actual_labels = [1 if rating > 0 else 0 for rating in actual_ratings]\n",
    "        predicted_labels = [1 if rating >= threshold else 0 for rating in predicted_ratings]\n",
    "\n",
    "        precision = precision_score(actual_labels, predicted_labels)\n",
    "        recall = recall_score(actual_labels, predicted_labels)\n",
    "        f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "        return rmse, precision, recall, f1\n",
    "\n",
    "user_item_matrix_sparse = csr_matrix(trainset_matrix.values)\n",
    "\n",
    "# Calculate global mean, user means, and item means\n",
    "global_mean = trainset_matrix.values.mean()\n",
    "user_means = trainset_matrix.mean(axis=1)\n",
    "item_means = trainset_matrix.mean(axis=0)\n",
    "\n",
    "# Start the recommendation system with the training data and additional parameters\n",
    "recSystrain = CollaborativeFilteringRecommendationSystem(testset_matrix, global_mean, user_means, item_means)\n",
    "\n",
    "# Get recommendations for a user\n",
    "user_id = 177458\n",
    "bestBook_recommendations = recSystrain.bookUser_Recommender(user_id)\n",
    "\n",
    "# Extract only ISBNs from the list of tuples\n",
    "isbn_Recommend = list(map(lambda x: x[0], bestBook_recommendations))\n",
    "\n",
    "# Extract book details for the best recommendations using ISBNs\n",
    "book_info = books_data.set_index('ISBN')\n",
    "recommended_books_info = book_info.loc[isbn_Recommend]\n",
    "\n",
    "# Display the best recommendations for the user\n",
    "display(recommended_books_info[['Book-Title', 'Year-Of-Publication']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649c89f",
   "metadata": {},
   "source": [
    "## Collaborative Filtering - Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee9e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate recommendations\n",
    "rmse, precision, recall, f1 = recSystrain.evaluate_recommendations(user_id, testset_matrix)\n",
    "\n",
    "print(\"Evaluation Metrics for Collaborative Filtering:\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257d8d3",
   "metadata": {},
   "source": [
    "## Content-Based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df75b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries based on 'Book-Title' and keep the first occurrence\n",
    "merged_data.drop_duplicates(subset='Book-Title', keep='first', inplace=True)\n",
    "\n",
    "# Class for Content-Based Filtering\n",
    "class ContentBasedFiltering:\n",
    "    def __init__(self, trainset, testset):\n",
    "        self.trainset = trainset  # Training set\n",
    "        self.testset = testset    # Test set\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(stop_words='english')  # TF-IDF vectorizer\n",
    "        self.tfidf_matrix = self._create_tfidf_matrix()  # Create TF-IDF matrix\n",
    "\n",
    "    # Method to create TF-IDF matrix\n",
    "    def _create_tfidf_matrix(self):\n",
    "        # Combine 'Book-Title' and 'Book-Author' as text data\n",
    "        book_text_data = self.trainset['Book-Title'].astype(str) + ' ' + self.trainset['Book-Author'].astype(str)\n",
    "        # Generate TF-IDF matrix\n",
    "        tfidf_matrix = self.tfidf_vectorizer.fit_transform(book_text_data)\n",
    "        return tfidf_matrix\n",
    "\n",
    "    # Method to recommend similar books\n",
    "    def recommend_similar_books(self, book_title, top_n=10):\n",
    "        book_indices = self.trainset[self.trainset['Book-Title'] == book_title].index  # Find index of the book title\n",
    "        if len(book_indices) == 0:\n",
    "            print(f\"Book '{book_title}' not found in the training set.\")\n",
    "            return []  # Return empty list if book not found\n",
    "        else:\n",
    "            book_index = book_indices[0]\n",
    "            similarity_scores = cosine_similarity(self.tfidf_matrix[book_index], self.tfidf_matrix)  # Calculate cosine similarity\n",
    "            similar_books_indices = similarity_scores.argsort(axis=1)[0][-top_n:][::-1]  # Find top similar books\n",
    "            similar_books_info = self.trainset.iloc[similar_books_indices][[\"ISBN\", \"Book-Title\", \"Book-Author\", \"Year-Of-Publication\"]]  # Retrieve book info\n",
    "            return similar_books_info\n",
    "\n",
    "    # Method to evaluate the content-based system\n",
    "    def evaluate(self, testset):\n",
    "        # Get indices of test set\n",
    "        test_book_indices = testset.index\n",
    "\n",
    "        # Create TF-IDF matrix for test set\n",
    "        test_book_text_data = testset['Book-Title'].astype(str) + ' ' + testset['Book-Author'].astype(str)\n",
    "        test_tfidf_matrix = self.tfidf_vectorizer.transform(test_book_text_data)\n",
    "\n",
    "        # Calculate similarities between test set and training set\n",
    "        similarities = cosine_similarity(test_tfidf_matrix, self.tfidf_matrix)\n",
    "\n",
    "        # Initialize evaluation metrics\n",
    "        top_n = 10\n",
    "        rmses = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1_scores = []\n",
    "\n",
    "        # Evaluate recommendations for each test item\n",
    "        for i, index in enumerate(test_book_indices):\n",
    "            similarity_scores = similarities[i]\n",
    "\n",
    "            top_similar_indices = similarity_scores.argsort()[-top_n:][::-1]\n",
    "\n",
    "            # Handle empty recommendations\n",
    "            if len(top_similar_indices) == 0:\n",
    "                continue\n",
    "\n",
    "            true_ratings = testset.loc[index]['Book-Rating']  # Get true ratings from test set\n",
    "            predicted_ratings = self.trainset.iloc[top_similar_indices]['Book-Rating'].mean()  # Predicted ratings from similar items\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = mean_squared_error([true_ratings], [predicted_ratings], squared=False)\n",
    "            rmses.append(rmse)\n",
    "\n",
    "            # Calculate precision, recall, and F1-score\n",
    "            intersection = len(set(top_similar_indices).intersection(set(self.trainset.index)))\n",
    "            precision = intersection / top_n\n",
    "            recall = intersection / len(self.trainset)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        # Calculate average evaluation metrics\n",
    "        if len(rmses) == 0:\n",
    "            avg_rmse = 0\n",
    "        else:\n",
    "            avg_rmse = sum(rmses) / len(rmses)\n",
    "\n",
    "        if len(precisions) == 0 or len(recalls) == 0:\n",
    "            avg_precision = 0\n",
    "            avg_recall = 0\n",
    "            avg_f1_score = 0\n",
    "        else:\n",
    "            avg_precision = sum(precisions) / len(precisions)\n",
    "            avg_recall = sum(recalls) / len(recalls)\n",
    "            avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "        return avg_rmse, avg_precision, avg_recall, avg_f1_score\n",
    "\n",
    "# Perform train-test split on merged_data\n",
    "trainset, testset = train_test_split(merged_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize content-based system with train and test sets\n",
    "content_based_system = ContentBasedFiltering(trainset, testset)\n",
    "\n",
    "# Get content-based recommendations for a specific book title\n",
    "book_title = \"Wild Justice\"\n",
    "bestBook_recommendations = content_based_system.recommend_similar_books(book_title)\n",
    "\n",
    "# Extract only ISBNs from the list of recommendations\n",
    "isbn_Recommend = bestBook_recommendations['ISBN'].tolist()\n",
    "\n",
    "# Extract book details for the recommended ISBNs from books_data\n",
    "book_info = books_data.set_index('ISBN')\n",
    "recommended_books_info = book_info.loc[isbn_Recommend]\n",
    "\n",
    "# Display the recommended book details\n",
    "display(recommended_books_info[['Book-Title', 'Year-Of-Publication']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cdf791",
   "metadata": {},
   "source": [
    "## Content-based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae48ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate recommendations\n",
    "avg_rmse, avg_precision, avg_recall, avg_f1_score = content_based_system.evaluate(testset)\n",
    "print(\"RMSE:\", avg_rmse)\n",
    "print(\"Precision:\", avg_precision)\n",
    "print(\"Recall:\", avg_recall)\n",
    "print(\"F1-score:\", avg_f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78583686",
   "metadata": {},
   "source": [
    "## Hybrid Recomendation System and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f91315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRecommender:\n",
    "    def __init__(self, collaborative_filter, content_filter, popular_items, collab_weight=8.5, content_weight=8.5):\n",
    "        self.collaborative_filter = collaborative_filter\n",
    "        self.content_filter = content_filter\n",
    "        self.popular_items = popular_items  # List of popular items as a fallback\n",
    "        self.collab_weight = collab_weight  # Weight for collaborative filtering scores\n",
    "        self.content_weight = content_weight  # Weight for content-based filtering scores\n",
    "\n",
    "    def hybrid_recommendations(self, user_id, book_title, testset_matrix, top_n=10):\n",
    "        # Generate collaborative filtering recommendations\n",
    "        collab_recs = self.collaborative_filter.bookUser_Recommender(user_id)\n",
    "\n",
    "        # Generate content-based recommendations\n",
    "        content_recs = self.content_filter.recommend_similar_books(book_title)\n",
    "\n",
    "        # Normalize scores from collaborative filtering recommendations\n",
    "        collab_scores = {isbn: rating for isbn, rating in collab_recs}\n",
    "        max_collab_score = max(collab_scores.values())\n",
    "        normalized_collab_scores = {isbn: score / max_collab_score for isbn, score in collab_scores.items()}\n",
    "\n",
    "        # Normalize scores from content-based recommendations\n",
    "        content_scores = {isbn: 1 / (index + 1) for index, isbn in enumerate(content_recs['ISBN'])}\n",
    "        max_content_score = max(content_scores.values())\n",
    "        normalized_content_scores = {isbn: score / max_content_score for isbn, score in content_scores.items()}\n",
    "\n",
    "        # Combine normalized scores with weighted average\n",
    "        combined_scores = {}\n",
    "        for isbn in set(normalized_collab_scores) | set(normalized_content_scores):\n",
    "            collab_score = normalized_collab_scores.get(isbn, 0) * self.collab_weight\n",
    "            content_score = normalized_content_scores.get(isbn, 0) * self.content_weight\n",
    "            combined_scores[isbn] = collab_score + content_score\n",
    "\n",
    "        # Sort the combined recommendations by score\n",
    "        sorted_combined_recs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:int(top_n)]\n",
    "\n",
    "        # Extract ISBNs from the combined recommendations\n",
    "        isbn_recommendations = [isbn for isbn, _ in sorted_combined_recs]\n",
    "\n",
    "        # Fallback for new users or items - Recommend popular items if no recommendations\n",
    "        if not isbn_recommendations:\n",
    "            isbn_recommendations = self.popular_items\n",
    "\n",
    "        # Extract book details for the recommended ISBNs from books_data\n",
    "        book_info = books_data.set_index('ISBN')\n",
    "        recommended_books_info = book_info.loc[isbn_recommendations]\n",
    "\n",
    "        # Evaluate performance using testset_matrix\n",
    "        user_test_ratings = testset_matrix.loc[user_id]\n",
    "        print(\"User Test Ratings:\")\n",
    "        print(user_test_ratings)\n",
    "\n",
    "        # Check if testset_matrix only contains zeros\n",
    "        print(\"Testset Matrix contains only zeros:\", user_test_ratings.eq(0).all().all())\n",
    "\n",
    "        # Print non-zero entries in user_test_ratings\n",
    "        non_zero_ratings = user_test_ratings[user_test_ratings != 0]\n",
    "        print(\"Non-Zero Ratings:\")\n",
    "        print(non_zero_ratings)\n",
    "\n",
    "        actual_ratings = user_test_ratings[user_test_ratings.index.isin(isbn_recommendations)]\n",
    "        print(\"Filtered Actual Ratings:\")\n",
    "        print(actual_ratings)\n",
    "\n",
    "        predicted_ratings = [combined_scores.get(isbn, 0) for isbn in isbn_recommendations]\n",
    "\n",
    "        # Calculate RMSE on test set\n",
    "        rmse = mean_squared_error(actual_ratings, predicted_ratings, squared=False)\n",
    "\n",
    "        # Calculate Precision, Recall, and F1-score\n",
    "        threshold = 0.1 \n",
    "        predicted_labels = [1 if rating >= threshold else 0 for rating in predicted_ratings]\n",
    "        actual_labels = [1 if rating > 0 else 0 for rating in actual_ratings]\n",
    "\n",
    "        precision = precision_score(actual_labels, predicted_labels)\n",
    "        recall = recall_score(actual_labels, predicted_labels)\n",
    "        f1 = f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "        print(\"Actual Ratings:\", actual_ratings)\n",
    "        print(\"Predicted Ratings:\", predicted_ratings)\n",
    "        print(\"Predicted Labels:\", predicted_labels)\n",
    "\n",
    "        predicted_ratings_train = [combined_scores.get(isbn, 0) for isbn in trainset_matrix.columns]\n",
    "        actual_ratings_train = trainset_matrix.loc[user_id]\n",
    "        rmse_train = mean_squared_error(actual_ratings_train, predicted_ratings_train, squared=False)\n",
    "\n",
    "        # Calculate Precision, Recall, and F1-score for the training set\n",
    "        predicted_labels_train = [1 if rating >= threshold else 0 for rating in predicted_ratings_train]\n",
    "        actual_labels_train = [1 if rating > 0 else 0 for rating in actual_ratings_train]\n",
    "\n",
    "        precision_train = precision_score(actual_labels_train, predicted_labels_train)\n",
    "        recall_train = recall_score(actual_labels_train, predicted_labels_train)\n",
    "        f1_train = f1_score(actual_labels_train, predicted_labels_train)\n",
    "\n",
    "        print(\"\\nTraining Set Metrics:\")\n",
    "        print(\"RMSE:\", rmse_train)\n",
    "        print(\"Precision:\", precision_train)\n",
    "        print(\"Recall:\", recall_train)\n",
    "        print(\"F1-score:\", f1_train)\n",
    "\n",
    "        return recommended_books_info[['Book-Title', 'Year-Of-Publication']], rmse, precision, recall, f1,rmse_train,precision_train,recall_train,f1_train\n",
    "\n",
    "collaborative_filter = CollaborativeFilteringRecommendationSystem(trainset_matrix, global_mean, user_means, item_means)\n",
    "\n",
    "content_filter = ContentBasedFiltering(trainset, testset)\n",
    "\n",
    "# List of popular items as a fallback (Books with ratings of 10)\n",
    "popular_items = [\n",
    "    '3596151465',\n",
    "    '055310666X',\n",
    "    '60096195',\n",
    "    '142302198',\n",
    "    '038076041X',\n",
    "    '699854289',\n",
    "    '786817070',\n",
    "    '805057706',\n",
    "    '1573248533',\n",
    "    '3423071516'\n",
    "]\n",
    "\n",
    "# Initialize the hybrid recommender system with both filters and popular items\n",
    "hybrid_recommender = HybridRecommender(collaborative_filter, content_filter, popular_items)\n",
    "\n",
    "# Get hybrid recommendations for a specific user and book title\n",
    "user_id = 177458\n",
    "book_title = \"Wild Justice\"\n",
    "# recommended_books, rmse, precision, recall, f1 = hybrid_recommender.hybrid_recommendations(user_id, book_title, testset_matrix)\n",
    "recommended_books, rmse, precision, recall, f1,rmse_train,precision_train,recall_train,f1_train = hybrid_recommender.hybrid_recommendations(user_id, book_title, trainset_matrix)\n",
    "\n",
    "# Display the hybrid recommendations and performance metrics\n",
    "#test set\n",
    "print(\"Recommended Books:\")\n",
    "display(recommended_books)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "\n",
    "#overfitting\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "print(\"RMSE:\", rmse_train)\n",
    "print(\"Precision:\", precision_train)\n",
    "print(\"Recall:\", recall_train)\n",
    "print(\"F1-score:\", f1_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
